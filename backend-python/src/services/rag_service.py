"""

RAG (Retrieval Augmented Generation) ì„œë¹„ìŠ¤

- PDF -> í…ìŠ¤íŠ¸ ì¶”ì¶œ/í´ë¦¬ë‹/ì²­í‚¹/ì¤‘ë³µì œê±°

- ì„ë² ë”© ìƒì„± + (ì˜µì…˜) PCA ì°¨ì› ì¶•ì†Œ + FAISS ê²€ìƒ‰

"""



import logging
import os
import re
from pathlib import Path

from typing import List, Dict, Any, Optional, Tuple



try:
    import faiss
    _FAISS_AVAILABLE = True
except Exception:
    faiss = None
    _FAISS_AVAILABLE = False
import numpy as np

try:
    from pypdf import PdfReader
except Exception:
    PdfReader = None

try:
    from chandra.input import load_file as chandra_load_file
    from chandra.model import InferenceManager
    from chandra.model.schema import BatchInputItem
    _CHANDRA_AVAILABLE = True
except Exception:
    chandra_load_file = None
    InferenceManager = None
    BatchInputItem = None
    _CHANDRA_AVAILABLE = False


from src.services.embedding_service import clean_text, chunk_text, deduplicate
from langchain_core.prompts import PromptTemplate


logger = logging.getLogger(__name__)

_chandra_manager = None
_chandra_manager_method = None


def _get_chandra_manager(method: str):
    global _chandra_manager, _chandra_manager_method
    if _chandra_manager is None or _chandra_manager_method != method:
        _chandra_manager = InferenceManager(method=method)
        _chandra_manager_method = method
    return _chandra_manager


def _extract_pdf_text_chandra(pdf_path: Path, method: str) -> List[str]:
    if not _CHANDRA_AVAILABLE:
        return []
    try:
        images = chandra_load_file(str(pdf_path), {})
        if not images:
            return []
        manager = _get_chandra_manager(method)
        batch = [BatchInputItem(image=img, prompt_type="ocr_layout") for img in images]
        results = manager.generate(
            batch,
            include_images=False,
            include_headers_footers=False,
        )
        page_texts = []
        for result in results:
            text = (result.markdown or "").strip()
            if text:
                page_texts.append(text)
        return page_texts
    except Exception as e:
        logger.warning(f"Chandra OCR failed for {pdf_path}: {e}")
        return []


def _load_text_file(filename: str, fallback: str = "") -> str:
    text_path = Path(__file__).with_name(filename)
    try:
        if text_path.exists():
            return text_path.read_text(encoding="utf-8")
    except Exception as e:
        logger.warning(f"Failed to read {filename}: {e}")
    return fallback



def _get_embedding_model():
    try:
        from src.models.model_manager import get_embedding_model

        return get_embedding_model()
    except Exception as e:
        logger.warning(f"Embedding model unavailable: {e}")
        return None


def _get_llm_model():
    try:
        from src.models.model_manager import get_llm_model

        return get_llm_model()
    except Exception as e:
        logger.warning(f"LLM model unavailable: {e}")
        return None


def _simple_hash_embeddings(texts: List[str], dim: int = 512) -> np.ndarray:
    mat = np.zeros((len(texts), dim), dtype=np.float32)
    for i, text in enumerate(texts):
        for token in re.findall(r"\\S+", text.lower()):
            idx = hash(token) % dim
            mat[i, idx] += 1.0
    return mat


def _clean_context_ko(text: str) -> str:
    if not text:
        return ""
    cleaned = re.sub(r"[^0-9A-Za-z\u3131-\u318E\uAC00-\uD7A3\s.,;:!?()\-/Â·%]", " ", text)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return cleaned


# RAG ìƒíƒœ

_rag_system = {
    "index": None,
    "embeddings_norm": None,
    "metadatas": [],
    "chunks": [],
    "pca": None,
    "initialized": False,
    "dimension": None,
    "original_dimension": None,
    "use_faiss": False,
}




STATIC_TEXT = ""
_static_text_path = Path(__file__).with_name("static_manual_ko.txt")
_notice_cache_path = Path(__file__).with_name("kopo_notices_cache.txt")
if _static_text_path.exists():
    STATIC_TEXT = _static_text_path.read_text(encoding="utf-8")
if _notice_cache_path.exists():
    notice_text = _notice_cache_path.read_text(encoding="utf-8")
    STATIC_TEXT = f"{STATIC_TEXT}\n\n{notice_text}".strip()
if not STATIC_TEXT:
    STATIC_TEXT = "Static manual not available."

def create_rag_prompt(language: str = "ko") -> PromptTemplate:
    """RAG prompt."""
    if language == "ko":
        template = _load_text_file(
            "rag_prompt_ko.txt",
            fallback="""ë‹¹ì‹ ì€ ë¶„ë‹¹ìœµí•©ê¸°ìˆ êµìœ¡ì›ì˜ ê³µì‹ AI ìƒë‹´ì›ì…ë‹ˆë‹¤.
**ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ** ë‹µë³€í•˜ì„¸ìš”.
ì œê³µëœ ë¬¸ë§¥ ì •ë³´ë§Œì„ ì‚¬ìš©í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ë‚´ìš©ì€ í˜„ì¬ ìë£Œì— ì—†ìŠµë‹ˆë‹¤. êµí•™ì²˜(031-696-8803)ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context}

ì§ˆë¬¸: {question}
ë‹µë³€(í•œêµ­ì–´): """,
        )
    else:
        template = """Context from Bundang Polytechnic documents:

{context}

Answer in English only, concisely, based only on the context. If unknown, suggest contacting the admin office (031-696-8803).

Question: {question}
Answer:"""

    return PromptTemplate(template=template, input_variables=["context", "question"])


def generate_rag_response(query: str, language: str = "ko", k: int = 5) -> Dict[str, Any]:
    """RAG response."""
    docs = retrieve_documents(query, k=k)
    if not docs:
        not_found_ko = _load_text_file(
            "rag_not_found_ko.txt",
            fallback="No matching documents found. Please contact the admin office (031-696-8803).",
        )
        return {
            "response": not_found_ko if language == "ko" else "No documents found.",
            "source": "none",
            "language": language,
        }

    context = "\n\n".join([d["content"] for d in docs])
    if language == "ko":
        context = _clean_context_ko(context)
    prompt = create_rag_prompt(language)
    formatted = prompt.format(context=context, question=query)

    model = _get_llm_model()
    if not model:
        return {
            "response": context[:1000] + "...",
            "source": "rag_document",
            "documents": docs,
            "language": language,
        }

    # ì‘ë‹µ ì†ë„ ê°œì„ : max_tokensë¥¼ 256ìœ¼ë¡œ ì œí•œ (512 â†’ 256)
    output = model(
        formatted,
        max_tokens=256,  # ì‘ë‹µ ì†ë„ 2ë°° í–¥ìƒ
        temperature=0.3,
        top_p=0.9,
        repeat_penalty=1.1,
        echo=False,
    )

    response_text = output["choices"][0]["text"].strip()
    tokens_used = output.get("usage", {}).get("completion_tokens", 0)

    return {
        "response": response_text,
        "source": "rag_llm",
        "documents": docs,
        "language": language,
        "tokens_used": tokens_used,
    }


def _load_pdfs(pdf_paths: Optional[List[Path]] = None, include_static: bool = True) -> Tuple[List[str], List[Dict[str, Any]]]:

    """PDFë“¤ì„ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì½ì–´ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜. í•„ìš” ì‹œ STATIC_TEXTë„ í¬í•¨."""
    texts: List[str] = []
    metas: List[Dict[str, Any]] = []

    def split_static_text(text: str) -> List[Tuple[str, str]]:
        lines = text.strip().splitlines()
        sections: List[Tuple[str, str]] = []
        current_title = "static_manual"
        current_lines: List[str] = []
        for line in lines:
            stripped = line.strip()
            if stripped and (stripped.startswith("#") or re.match(r"^\\d+#", stripped)):
                if current_lines:
                    sections.append((current_title, "\n".join(current_lines).strip()))
                    current_lines = []
                current_title = stripped
            current_lines.append(line)
        if current_lines:
            sections.append((current_title, "\n".join(current_lines).strip()))
        return sections

    targets: List[Path] = []

    if pdf_paths:
        print(f"  ğŸ“Œ ì‚¬ìš©ì ì§€ì • PDF ê²½ë¡œ: {len(pdf_paths)}ê°œ")
        targets = pdf_paths

    else:
        print(f"  ğŸ” ìë™ PDF ê²€ìƒ‰ ì¤‘...")
        root_dir = Path(__file__).parent.parent.parent.parent  # repo root
        print(f"     ê²€ìƒ‰ ê²½ë¡œ: {root_dir}")

        candidates = list(root_dir.glob("*.pdf")) + list((root_dir / "backend-python").glob("*.pdf"))
        print(f"  ğŸ“ ë°œê²¬ëœ PDF íŒŒì¼: {len(candidates)}ê°œ")

        targets = [p for p in candidates if p.exists()]
        if targets:
            for pdf in targets:
                print(f"     - {pdf.name}")



    if not targets or (PdfReader is None and not _CHANDRA_AVAILABLE):
        if PdfReader is None and targets and not _CHANDRA_AVAILABLE:
            print(f"  âš ï¸ pypdf/chandra ì‚¬ìš© ë¶ˆê°€ - PDF ì¶”ì¶œ ê±´ë„ˆëœ€")
            logger.warning("No PDF reader available; skipping PDF extraction.")
        if include_static:
            print(f"  ğŸ“ Static manual ë¡œë“œ ì¤‘...")
            sections = split_static_text(STATIC_TEXT)
            print(f"  âœ… Static manual {len(sections)}ê°œ ì„¹ì…˜ ë¡œë“œ ì™„ë£Œ")
            for i, (title, section_text) in enumerate(sections, start=1):
                texts.append(section_text)
                metas.append(
                    {
                        "file": "static_manual",
                        "path": "static_manual",
                        "page": i,
                        "section": title,
                    }
                )
        return texts, metas

    if not targets:
        print(f"  âš ï¸ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        logger.warning("No PDF files found.")
        return texts, metas


    print(f"  ğŸ“– PDF íŒŒì¼ ì½ëŠ” ì¤‘...")
    pdf_page_count = 0
    for pdf_path in targets:

        try:
            chandra_enabled = os.getenv("CHANDRA_PDF_ENABLED", "false").lower() in (
                "1",
                "true",
                "yes",
            )
            chandra_method = os.getenv("CHANDRA_METHOD", "hf")
            chandra_texts = []
            if chandra_enabled and _CHANDRA_AVAILABLE:
                print(f"     - {pdf_path.name}: Chandra OCR ì‹œë„ ì¤‘...")
                chandra_texts = _extract_pdf_text_chandra(pdf_path, chandra_method)
                if chandra_texts:
                    print(f"     - {pdf_path.name}: Chandra OCR {len(chandra_texts)}í˜ì´ì§€ ì¶”ì¶œ")
                    for page_idx, page_text in enumerate(chandra_texts):
                        page_text = page_text.replace("\x00", " ").strip()
                        if not page_text:
                            continue
                        texts.append(page_text)
                        pdf_page_count += 1
                        metas.append(
                            {
                                "file": pdf_path.name,
                                "path": str(pdf_path),
                                "page": page_idx + 1,
                                "source": "chandra",
                            }
                        )
                    continue

            if PdfReader is None:
                print(f"     - {pdf_path.name}: pypdf ì‚¬ìš© ë¶ˆê°€, PDF ì¶”ì¶œ ê±´ë„ˆëœ€")
                continue
            reader = PdfReader(str(pdf_path))
            num_pages = len(reader.pages)
            print(f"     - {pdf_path.name}: {num_pages}í˜ì´ì§€")

            for page_idx, page in enumerate(reader.pages):

                try:

                    raw = page.extract_text() or ""

                except Exception:

                    raw = ""

                raw = raw.replace("\x00", " ").strip()

                if not raw:

                    continue

                texts.append(raw)
                pdf_page_count += 1

                metas.append(

                    {

                        "file": pdf_path.name,

                        "path": str(pdf_path),

                        "page": page_idx + 1,

                    }

                )

        except Exception as e:
            print(f"  âŒ PDF ì½ê¸° ì‹¤íŒ¨ ({pdf_path.name}): {e}")
            logger.error(f"Failed to read {pdf_path}: {e}")

            continue

    print(f"  âœ… PDFì—ì„œ {pdf_page_count}ê°œ í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")



    # ì¶”ê°€ ìŠ¤íƒœí‹± í…ìŠ¤íŠ¸ ì‚½ì…
    if include_static:
        print(f"  ğŸ“ Static manual ì¶”ê°€ ì¤‘...")
        sections = split_static_text(STATIC_TEXT)
        print(f"  âœ… Static manual {len(sections)}ê°œ ì„¹ì…˜ ì¶”ê°€ ì™„ë£Œ")
        for i, (title, section_text) in enumerate(sections, start=1):
            texts.append(section_text)
            metas.append(
                {
                    "file": "static_manual",
                    "path": "static_manual",
                    "page": i,
                    "section": title,
                }
            )

    print(f"  ğŸ“Š ì´ {len(texts)}ê°œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
    return texts, metas





def _build_embeddings(

    texts: List[str],

    metas: List[Dict[str, Any]],

    chunk_size: int = 800,

    chunk_overlap: int = 120,

    target_dim: Optional[int] = 256,

) -> Tuple[np.ndarray, List[str], List[Dict[str, Any]], Optional[Any]]:

    """

    í…ìŠ¤íŠ¸ -> ì²­í¬ -> ì„ë² ë”© -> (PCA) -> ì •ê·œí™” ë²¡í„°

    """

    print(f"  ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = _get_embedding_model()
    if embedding_model:
        print(f"  âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    else:
        print(f"  âš ï¸ ì„ë² ë”© ëª¨ë¸ ì—†ìŒ - í•´ì‹œ ì„ë² ë”© ì‚¬ìš©")

    all_chunks: List[str] = []

    all_meta: List[Dict[str, Any]] = []


    print(f"  âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
    for idx, text in enumerate(texts):

        cleaned = clean_text(text)

        chunks = chunk_text(cleaned, max_len=chunk_size, overlap=chunk_overlap)

        chunks = deduplicate(chunks)

        for chunk in chunks:

            all_chunks.append(chunk)

            meta_copy = dict(metas[idx]) if idx < len(metas) else {}

            meta_copy["source_index"] = idx

            meta_copy["text_length"] = len(chunk)

            all_meta.append(meta_copy)

    print(f"  âœ… ì²­í‚¹ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")



    if not all_chunks:
        print(f"  âš ï¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return np.empty((0, 0)), [], [], None


    print(f"  ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘ ({len(all_chunks)}ê°œ ì²­í¬)...")
    if embedding_model:
        emb_matrix = embedding_model.encode(
            all_chunks, convert_to_numpy=True, show_progress_bar=False
        )
        # sentence-transformers returns ndarray; HF wrapper returns list of arrays
        if isinstance(emb_matrix, list):
            emb_matrix = np.vstack(emb_matrix)
        print(f"  âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ (shape: {emb_matrix.shape})")
    else:
        emb_matrix = _simple_hash_embeddings(all_chunks, dim=512)
        print(f"  âœ… í•´ì‹œ ì„ë² ë”© ìƒì„± ì™„ë£Œ (shape: {emb_matrix.shape})")


    pca = None

    if target_dim and target_dim > 0:

        n_samples, n_features = emb_matrix.shape
        print(f"  ğŸ“ PCA ì°¨ì› ì¶•ì†Œ ê²€í†  ì¤‘ (í˜„ì¬: {n_features}D â†’ ëª©í‘œ: {target_dim}D)...")

        # ìƒ˜í”Œ ìˆ˜ê°€ ì¶©ë¶„í•  ë•Œë§Œ PCA ì ìš©. ë¶€ì¡±í•˜ë©´ ì›ë³¸ ì°¨ì›(ì˜ˆ: 1024)ì„ ìœ ì§€.

        if n_samples > target_dim and target_dim < n_features:

            try:
                from sklearn.decomposition import PCA

                effective_dim = min(target_dim, n_samples - 1)
                if effective_dim > 1:
                    pca = PCA(n_components=effective_dim, random_state=42)
                    emb_matrix = pca.fit_transform(emb_matrix)
                    print(f"  âœ… PCA ì ìš© ì™„ë£Œ ({n_features}D â†’ {emb_matrix.shape[1]}D)")
            except Exception as e:
                print(f"  âš ï¸ PCA ì ìš© ì‹¤íŒ¨: {e}")
                pca = None
        else:
            print(f"  âš ï¸ PCA ì¡°ê±´ ë¶ˆì¶©ì¡± (ìƒ˜í”Œ: {n_samples}, ì°¨ì›: {n_features})")


    # ì •ê·œí™” (ë‚´ì  ê¸°ë°˜ ê²€ìƒ‰)
    print(f"  ğŸ”„ ë²¡í„° ì •ê·œí™” ì¤‘...")
    norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True) + 1e-10

    emb_norm = emb_matrix / norms
    print(f"  âœ… ì •ê·œí™” ì™„ë£Œ")



    return emb_norm.astype("float32"), all_chunks, all_meta, pca





def initialize_rag_system(pdf_paths: Optional[List[str]] = None, target_dim: int = 256) -> bool:
    """PDFë¥¼ ì½ì–´ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±."""
    try:
        print("  ğŸ“„ PDF ë¬¸ì„œ ë¡œë”© ì¤‘...")
        paths = [Path(p) for p in pdf_paths] if pdf_paths else None
        texts, metas = _load_pdfs(paths, include_static=True)

        if not texts:
            print("  âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            if STATIC_TEXT:
                print("  ğŸ“ Static manualë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                texts = [STATIC_TEXT]
                metas = [{"file": "static_manual", "path": "static_manual", "page": 1}]
            else:
                logger.warning("No texts extracted from PDFs.")
                print("  âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: ë¬¸ì„œ ì—†ìŒ")
                return False
        else:
            print(f"  âœ… {len(texts)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

        print("  ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
        emb_norm, chunks, metadatas, pca = _build_embeddings(
            texts, metas, chunk_size=800, chunk_overlap=120, target_dim=target_dim
        )

        if emb_norm.size == 0:
            logger.warning("No embeddings built.")
            print("  âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False

        print(f"  âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

        dim = emb_norm.shape[1]
        index = None
        if _FAISS_AVAILABLE:
            print(f"  ğŸ” FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (ì°¨ì›: {dim})...")
            index = faiss.IndexFlatIP(dim)
            index.add(emb_norm)
            print(f"  âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
        else:
            print(f"  âš ï¸ FAISS ì‚¬ìš© ë¶ˆê°€ - numpy ê²€ìƒ‰ ì‚¬ìš© (ì°¨ì›: {dim})")

        _rag_system["index"] = index
        _rag_system["embeddings_norm"] = emb_norm
        _rag_system["metadatas"] = metadatas
        _rag_system["chunks"] = chunks
        _rag_system["pca"] = pca
        _rag_system["dimension"] = dim
        _rag_system["original_dimension"] = (
            pca.n_features_ if pca is not None else dim
        )
        _rag_system["initialized"] = True
        _rag_system["use_faiss"] = _FAISS_AVAILABLE

        logger.info(f"RAG initialized: chunks={len(chunks)}, dim={dim}")
        print(f"  ğŸ“Š RAG ì‹œìŠ¤í…œ í†µê³„:")
        print(f"     - ì´ ì²­í¬: {len(chunks)}")
        print(f"     - ì°¨ì›: {dim}")
        print(f"     - FAISS: {'ì‚¬ìš©' if _FAISS_AVAILABLE else 'ë¯¸ì‚¬ìš©'}")
        return True

    except Exception as e:
        logger.error(f"RAG initialization error: {e}", exc_info=True)
        print(f"  âŒ RAG ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False





def retrieve_documents(query: str, k: int = 5) -> List[Dict[str, Any]]:
    """ì¿¼ë¦¬ë¡œ ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜."""
    if not _rag_system["initialized"]:
        print(f"  âš ï¸ RAG ë¯¸ì´ˆê¸°í™” - ë¬¸ì„œ ê²€ìƒ‰ ë¶ˆê°€")
        logger.warning("RAG system not initialized, cannot retrieve documents")
        return []

    print(f"  ğŸ” ë¬¸ì„œ ê²€ìƒ‰: '{query}' (ìƒìœ„ {k}ê°œ)")



    embedding_model = _get_embedding_model()
    if not embedding_model:
        # Fallback to hash embeddings when no model is available.
        embedding_model = None


    try:

        if embedding_model:
            q_emb = embedding_model.encode([query], convert_to_numpy=True)
            if isinstance(q_emb, list):
                q_emb = np.array(q_emb[0])
            else:
                q_emb = q_emb[0]
        else:
            q_emb = _simple_hash_embeddings([query], dim=_rag_system["dimension"] or 512)[0]
        if _rag_system["pca"] is not None:

            q_emb = _rag_system["pca"].transform(q_emb.reshape(1, -1))[0]

        q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-10)



        index = _rag_system["index"]
        emb_norm = _rag_system["embeddings_norm"]
        if _rag_system["use_faiss"] and index is not None:
            scores, idxs = index.search(np.array([q_emb], dtype="float32"), k)
            score_list = scores[0]
            idx_list = idxs[0]
        else:
            if emb_norm is None or len(emb_norm) == 0:
                return []
            scores_all = emb_norm @ q_emb
            k = min(k, scores_all.shape[0])
            idx_list = np.argsort(-scores_all)[:k]
            score_list = scores_all[idx_list]

        results = []
        for score, idx in zip(score_list, idx_list):
            if idx < 0 or idx >= len(_rag_system["chunks"]):
                continue
            results.append(

                {

                    "content": _rag_system["chunks"][idx],

                    "metadata": _rag_system["metadatas"][idx],

                    "score": float(score),

                }

            )

        return results

    except Exception as e:

        logger.error(f"Document retrieval error: {e}")

        return []





def is_rag_initialized() -> bool:

    return _rag_system["initialized"]





def get_vector_store():

    return _rag_system["index"]

